---
title: "SI 618 Fall 2014, Homework 4"
output: html_document
---

**Due Wed. Nov. 26, 2014. 5:30pm**

In this homework, you'll practice using hierarchical and k-means-family clustering on an automotive dataset.  The file **cars.tsv** is provided in the zip file.  You'll also need to load the 'gplots' and 'cluster' libraries. Note that many clustering-related functions in R either produce graphical output themselves, or produce objects that work directly with R's built-in plot() function. So unlike other assignments, you don't need to use ggplot() here to create these plots: just plot() will do.

#### Part 1. [20 points] Data preparation

To prepare for clustering, you need to scale the data: Do this for the **cars** dataset by calling the appropriate R scaling function: use settings so that each variable (column) is centered by subtracting the variable (column) mean, and scaled by dividing by the variable's standard deviation. Use the car names for the data frame row names.

(a) Show the first 5 rows of the scaled data frame, and 

```{r, echo=FALSE, fig.width=14}
setwd("/Users/rita/Google Drive/618/homework/assignment_files/hw10/si618-w17-hw10")
cars<- read.table("cars.tsv",header = TRUE, sep='\t', quote = "", row.names = NULL,  comment.char = "",fill = TRUE,fileEncoding = "UTF-8-BOM")
rownames(cars)<- cars$Car
cars<- cars[, !(colnames(cars) %in% c("Car","Country"))]
cars<- scale(cars)
head(cars)
```


(b) Compute a distance object based on the Euclidean distance between the rows of the scaled dataset. Convert the distance object to a matrix and show the 5x5 upper corner of the matrix (i.e. containing the first 5 rows and columns).

```{r, echo=FALSE, fig.width=14}
cars.dist<- dist(cars,method = "euclidean")
cars.d.final<- as.matrix(cars.dist)
head(cars.d.final)
```

#### Part 2. [20 points] Hierarchical clustering. 
Using the distance object you computed from 1(b), compute and plot a hierarchical cluster analysis using average-linkage clustering. With this clustering, cut the tree into 3 clusters and plot the dendogram with red borders around the clusters (Hint: use rect.hclust() function).

```{r, echo=FALSE, fig.width=12}
cluster<- kmeans(cars,3)
cars.hclust<- hclust(cars.dist,method="average")
cars<- as.data.frame(cars)
plot(cars.hclust,labels=cars$Car,main="Hierarchical cluster analysis using average linkage")
```

#### Part 3. [10 points] Using clustering results

The output from the tree-cutting function in 2(b) above should produce a mapping of car type to cluster number (from 1 to 3), like this:
```{r, echo=TRUE}
groups<-cutree(cars.hclust, k=3)
groups

```


With this group mapping, produce three tables:

a) a 1-dimensional contingency table showing the number of cars in each cluster;

b) a 2-dimensional contingency table of the number of cars in each cluster from each country of manufacture; and

c) a table showing the median value per cluster of each variable.

The desired output is shown here:

```{r, echo=FALSE}
groups.3<- groups
groups.3<- as.factor(groups.3)
table(groups.3)
table(groups.3,names(groups.3))
cars<- read.table("cars.tsv",header = TRUE, sep='\t', quote = "", row.names = NULL,  comment.char = "",fill = TRUE,fileEncoding = "UTF-8-BOM")
rownames(cars)<- cars$Car
aggregate(cars[,-c(1,2)],list(groups.3),median)
table(groups.3,cars$Country)
```

#### Part 4. Heatmaps [10 points]

Use the heatmap.2 function to produce a heatmap of the cars dataset with these settings:

- average-link clustering

- column-based scaling

- row-based dendrogram
e
- no density info

You do not need to reproduce the exact width and height shown here, but for reference the example used these settings:

margins = c(5, 8), cexRow=0.7,cexCol=0.7.

```{r, echo=FALSE}
library(gplots)
gplots::heatmap.2(as.matrix(cars),hclustfun = function(x) 
  hclust(x,method="average"),scale="column",dendrogram = "row",trace="none",density.info="none",col=redblue(256),lhei=c(2,5.0), lwid=c(1.5,2.5),
keysize = 0.25,
margins = c(5, 8),
cexRow=0.7,cexCol=0.7)
```

#### Part 5. [20 points] k-medoids clustering.

Apply the `partitioned around medoids' R function to the distances you computed in 1(b) to find three clusters of cars.  

(a) Compare this to the 3 clusters you found with heirarchical clustering in Part 2, by showing the 2-dimensional contingency table for the hierarchical group variable (shown in Part 3) vs. the clustering variable that is output by the 'partitioned around medoids' function (Part 4).  How well do the two clusterings agree?  (**include your answers as output in your version of this report**)

(b) Give the medoid car found for each cluster. (**include your answers as output in your version of this report**)

(c) Show the k-medoids clusters from 5(a) using the appropriate bivariate cluster plotting function, as shown.

```{r, echo=FALSE, fig.height = 10}
p_cluster<- pam(cars.d.final,k=3)
p_clusters<- as.factor(p_cluster$clustering)
h_cluster_t<- table(groups.3,names(groups.3))
p_cluster_t <- table(p_clusters,names(p_clusters))
#their agreeness
table(groups.3,p_cluster$clustering)
cars$Car[p_cluster$id.med]
library(cluster)
clusplot(cars.d.final,p_cluster$clustering,color=TRUE, shade=TRUE, labels=2, lines=0)
```

#### Part 6. [15 points] Assessing cluster quality.

Create a silhouette plot based on the k-medoid clusters found in Part 5 and distance matrix from Part 1. 
What can you conclude from the plot about the quality of these three clusters? (**include your answer as output in your version of this report**)
```{r, echo=FALSE, fig.height = 10}
plot(p_cluster)
```

